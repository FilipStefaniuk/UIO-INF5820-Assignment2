%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Lachaise Assignment
% LaTeX Template
% Version 1.0 (26/6/2018)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Marion Lachaise & François Févotte
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

	
\input{structure.tex} % Include the file specifying the document structure and custom commands
\usepackage{listings}
\usepackage{booktabs}

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Assignment 2: Distributional\\ Word Embeddings} % Title of the assignment

\author{Filip Stefaniuk\\ \texttt{filipste@student.matnat.uio.no}} % Author name and email address

\date{\today} % University, school and/or department name(s) and a date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------
\section*{Introduction}
This assignment had four different parts, each is described in detail in separate section of this report.
To solve the problems I have implemented a number of scripts:
\begin{itemize}
	\item \lstinline{similar_wv.py} - finds nearest neighbours of the list of given words
	\item \lstinline{eval_wv_intinsic.py} - evaluates models on intrinsic evaluation datasets
	\item \lstinline{train_word2vec.py} - trains word embeddings model
	\item \lstinline{train_model.py} - trains neural network model for calssifying texts
	\item \lstinline{eval_on_test.py} - evaluates model on test data
\end{itemize}
Every script has interface with support of \lstinline{--help} command, that shortly explains script and
other command line arguments. Most of the results are saved in json files. The code snippets to
preprocess the data and process the results are in notebooks. Scripts, notebooks, results and
latex source for this report are available in github repository.
\section{Basic operations with word embeddings}

For this part of the assignment, I have used script \lstinline{play_with_model.py} which I
have slightly modified, to allow me specifiy the range of indexes of the nearest words.

To produce input file, I copied the first sentence from Chen and Manning paper, and used
nltk to lemmatize, tag and remove stop words from that sentence. Results are saved in the file
\lstinline{data/words_auto.txt}. Unfortunately not everything was done correctly, namely one
word was badly lemmatized and one had wrong POS tag. I manually
corrected the mistakes and saved it as \lstinline{data/words.txt}.

Finally I run the script using 2 different word models \textit{Wikipedia} and \textit{Gigaword}. I saved
the results in \lstinline{results/similar_words/}. Some of the words are presented in the tables below:

\begin{figure}[h]	
	\begin{center}
		\textbf{Top Words: classify\_VERB} \par \medskip
		\scalebox{1}{\input{./similarities_classify.tex}}
		\caption{11th to 15th nearest neighbours for word 'classify', with corresponding scores.}
	\end{center}
\end{figure}

\begin{figure}[h]	
	\begin{center}
		\textbf{Top Words: current\_ADJ} \par \medskip
		\scalebox{1}{\input{./similarities_current.tex}}
		\caption{11th to 15th nearest neighbours for word 'current', with corresponding scores.}
	\end{center}
\end{figure}

It looks like \textit{Wikipedia} model captures the meaning of 'classify' quite well with the
similar words like 'group' or 'label'. Moreover, becouse the words presented above 
are not the closest neighbours it is also interesting to see that the other meaning of 'classify' was captured with the words such as 'nonpublic' and 'classified'.
With word 'current' another phenomena occures, while in \textit{Gigaword} we can see words with quite similar meanining,
in \textit{Wikipedia} we observe the dates! This is not wrong, since when talking about \textit{current} events, we often
think about last days or years, but this example illustrates that with anomalies like this one it is difficult to train models, that
will stay valid for a longer period of time.

\section{Intrinsic evaluation of pre-trained word embeddings}
I have downloaded both \textbf{SimLex999} and \textbf{Google Analogies} datasets,
and I evaluated on them three different models of word embeddings. I have used builtin gensim
methods \lstinline{evaluate_word_pairs()} and \lstinline{evaluate_word_analogies()}.

\subsection{SimLex999}
I fund out that, the simplest way to split \textbf{SimLex999} into sections, was to simply split
dataset into separate files. Because there were no clear separators between sections I have checked
the original paper and discovered, that the dataset is structured in the way that the first 111 word pairs
are adjectives next 666 are nouns, and the last 222 are verbs. I split the dataset accordingly.

\begin{figure}[h]
	\centering
	\input{./simlex_table_1.tex}
	\caption{Pearson correlation on SimLex999 dataset.}
\end{figure}
\begin{figure}[h]
	\centering
	\input{./simlex_oov_table_1.tex}
	\caption{Ratio of OOV words (\%).}
\end{figure}

From the evaluation results we can observe that comparing verb pairs is the most difficult.
It is very interesting, since the authors of the \textbf{Simlex999} mentioned in their work,
that when testing word pairs with children, they also had the most difficulty with the verbs.
It is also worth mentioning that even though the second model (trained on Oil and Gas data)
achieved better correlation on verbs than the other two, 30\% of the words were not in the vocabulary.

\subsection{Google Analogies}
Gensim methods for evaluation are not very flexible, and with word analogies
there was no simple method to extract the information about oov tokens, that was logged but not
returned as a result from function. I present the results of the evaluation below, but
I want to emphesise that in case of the second model 35\% of words were not in vocabulary.

\begin{figure}[h]
	\centering
	\input{./analogies_table_1.tex}
	\caption{Accuracy scores, evaluation on Google Analogies dataset.}
\end{figure}


\subsection{Analysis}
On both test sets the clear winner is the model trained on \textit{Common Crawl}. There is a
simple explanation for that fact, this model was trained on the largest number of data which really
matters in case of word embeddings. 

\section{ Training a word embedding model on in-domain data}
I have trained two models using \lstinline{train_word2vec.py}, the models were trained
with skipGram algorithm, window size of 2 and vector sizes of \textbf{300} (model1) and \textbf{100} (model2).
Both models were trained for 2 iterations.

\subsection{Top most frequent words}
I have compared the most frequent words from models trained on \textit{SignalMedia},
\textit{CoNLL17}, \textit{Oil and Gas} and \textit{Common Crawl}. The most common words in models trained on large corporas are either interpunction signs
or stop words. The explanation is that the large amount of data is more difficult to process
and the quality of it drops. The models achieve good results because the amount of data
compensates its quality. In report I include only 10 most common but file \lstinline{results/top_words.csv}
contains all 20.


\begin{figure}
	\centering
	\scalebox{0.92}{\input{./top_words.tex}}
	\caption{Most common words in models}
\end{figure}

\clearpage

\subsection{Intrinsic Evaluation}
I have evaluated both models in the same manner as the models in the previous section:
\begin{figure}[h]
	\centering
	\begin{minipage}{0.45\textwidth}
	\input{./simlex_table_2.tex}
	\end{minipage}
	\begin{minipage}{0.45\textwidth}
	\centering
	\input{./simlex_oov_table_2.tex}
	\end{minipage}
	\caption{Pearson correlation and percentage of OOV words on SimLex999 dataset.}
\end{figure}

Interestingly correlation on nouns were as good as in \textit{Common Crawl} model, even
though this model was trained on very little data. The total correlation is not really comparable,
since 90\% of verbs, which loweres the result were out of vocabulary. Comparing two models trained
on in-domain data it can be seen that increasing vector size improves the results.

\begin{figure}[h]
	\centering
	\input{./analogies_table_2.tex}
	\caption{Accuracy scores, evaluation on Google Analogies dataset.}
\end{figure}

It is hard to get useful information from evaluating the models on analogies dataset,
since almost all the classes contained words that were not in the vocabulary of this models.
The only meaningful result was from evaluating on \textit{family} section, on which model performes
worse than pre-trained embeddings. The reason for that is that the domain on which the model was 
trained is much different from the categories in this dataset.

\section{Document classification with word embeddings}

\subsection{Training a classifier}
I have trained a model to classifiy articles from SignalMedia dataset.
After couple of experiments, I have ended up with the feed forward network with
3 hidden layers of 256, 128 and 64 neurons. I have used Adam optimizer and
categorical crossentropy loss function. I have tokenized a teksts, used only 3000 most
common words. I decided that using all the words would provide unnecessairy noise
when trainig embeddings. When using pretrained models most of these words
weren't in the dictionairy either way. Then I padded the sequences
to 1000. I have build embedding manually layer, gensim function since it is not
compatible with keras \lstinline{pad_sequences}.

I have tested the model with all the available embeddings but in most cases
results were bad. I believe this is due to the fact that articles are long, and
simple average of embeddings may be very similar regardless of the source of the article.
When using BOW, some website may have used specific vocabulary, when using embeddings,
when two different sources write similar article with different words but the same
semantical information they are very difficult to distinguish. Maybe that is the reason
why model with BOW had better results that the ones with word embeddings.

The best performing models that use embeddings were the one with trained embeddings during
classification task and the one trained on Signal Dataset. I have tried training different
embedding model with wider window but it didn't improve the results. 


\begin{figure}[h]
	\centering
	\input{./classification_results.tex}
	\caption{F1 score for classifier trained with different word embeddings}
\end{figure}

\subsection{Evaluation of the best model}
For my best model I decided to train embeddings during the classification task,
but use pretrained embeddings as initial values. That significantly improved results:

\begin{figure}[h]
	\centering
	\begin{minipage}{0.45\textwidth}
	\scalebox{0.7}{\input{./best_avg.tex}}
	\end{minipage}
	\begin{minipage}{0.45\textwidth}
	\centering
	\scalebox{0.7}{\input{./best_std.tex}}
	\end{minipage}
	\caption{average results from 3 runs of the best model.}
\end{figure}

\subsection{Conclusion}
I believe that in this task using embeddings didn't help at all. It was
difficult to even achieve results similar to those that I got using BOWs.
The main reason may be that taking a simple average of embeddings is not
good enough representation of the document. Better results may have been
achieved with taking weighted average or using neural network encoder.

\end{document}
