%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Lachaise Assignment
% LaTeX Template
% Version 1.0 (26/6/2018)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Marion Lachaise & François Févotte
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

	
\input{structure.tex} % Include the file specifying the document structure and custom commands
\usepackage{listings}
\usepackage{booktabs}

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Assignment 2: Distributional\\ Word Embeddings} % Title of the assignment

\author{Filip Stefaniuk\\ \texttt{filipste@student.matnat.uio.no}} % Author name and email address

\date{\today} % University, school and/or department name(s) and a date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------
\section*{Introduction}
This assignment had four different parts, each is described in detail in separate section of this report.
To solve the problems I have implemented a number of scripts:
\begin{itemize}
	\item \lstinline{similar_wv.py} - finds nearest neighbours of the list of given words
	\item \lstinline{eval_wv_intinsic.py} - evaluates models on intrinsic evaluation datasets
	\item \lstinline{train_word2vec.py} - trains word embeddings model
	\item \lstinline{train_model.py} - trains neural network model for calssifying texts
	\item \lstinline{eval_on_test.py} - evaluates model on test data
\end{itemize}
Every script has interface with support of \lstinline{--help} command, that shortly explains script and
other command line arguments. Most of the results are saved in json files. The code snippets to
preprocess the data and process the results are in notebooks. Scripts, notebooks, results and
latex source for this report are available in github repository.
\section{Basic operations with word embeddings}

For this part of the assignment, I have used script \lstinline{play_with_model.py} which I
have slightly modified, to allow me specifiy the range of indexes of the nearest words.

To produce input file, I copied the first sentence from Chen and Manning paper, and used
nltk to lemmatize, tag and remove stop words from that sentence. Results are saved in the file
\lstinline{data/words_auto.txt}. Unfortunately not everything was done correctly, namely one
word was badly lemmatized and one had wrong POS tag. I manually
corrected the mistakes and saved it as \lstinline{data/words.txt}.

Finally I run the script using 2 different word models \textit{Wikipedia} and \textit{Gigaword}. I saved
the results in \lstinline{results/similar_words/}. Some of the words are presented in the tables below:

\begin{figure}[h]	
	\begin{center}
		\textbf{Top Words: classify\_VERB} \par \medskip
		\scalebox{1}{\input{./similarities_classify.tex}}
		\caption{11th to 15th nearest neighbours for word 'classify', with corresponding scores.}
	\end{center}
\end{figure}

\begin{figure}[h]	
	\begin{center}
		\textbf{Top Words: current\_ADJ} \par \medskip
		\scalebox{1}{\input{./similarities_current.tex}}
		\caption{11th to 15th nearest neighbours for word 'current', with corresponding scores.}
	\end{center}
\end{figure}

It looks like \textit{Wikipedia} model captures the meaning of 'classify' quite well with the
similar words like 'group' or 'label'. Moreover, becouse the words presented above 
are not the closest neighbours it is also interesting to see that the other meaning of 'classify' was captured with the words such as 'nonpublic' and 'classified'.
With word 'current' another phenomena occures, while in \textit{Gigaword} we can see words with quite similar meanining,
in \textit{Wikipedia} we observe the dates! This is not wrong, since when talking about \textit{current} events, we often
think about last days or years, but this example illustrates that with anomalies like this one it is difficult to train models, that
will stay valid for a longer period of time.

\section{Intrinsic evaluation of pre-trained word embeddings}
I have downloaded both \textbf{SimLex999} and \textbf{Google Analogies} datasets,
and I evaluated on them three different models of word embeddings. I have used builtin gensim
methods \lstinline{evaluate_word_pairs()} and \lstinline{evaluate_word_analogies()}.

\subsection{SimLex999}
I fund out that, the simplest way to split \textbf{SimLex999} into sections, was to simply split
dataset into separate files. Because there were no clear separators between sections I have checked
the original paper and discovered, that the dataset is structured in the way that the first 111 word pairs
are adjectives next 666 are nouns, and the last 222 are verbs. I split the dataset accordingly.

\begin{figure}[h]
	\centering
	\input{./simlex_table_1.tex}
	\caption{Pearson correlation on SimLex999 dataset.}
\end{figure}
\begin{figure}[h]
	\centering
	\input{./simlex_oov_table_1.tex}
	\caption{Ratio of OOV words (\%).}
\end{figure}

From the evaluation results we can observe that comparing verb pairs is the most difficult.
It is very interesting, since the authors of the \textbf{Simlex999} mentioned in their work,
that when testing word pairs with children, they also had the most difficulty with the verbs.
It is also worth mentioning that even though the second model (trained on Oil and Gas data)
achieved better correlation on verbs than the other two, 30\% of the words were not in the vocabulary.

\subsection{Google Analogies}
Gensim methods for evaluation are not very flexible, and with word analogies
there was no simple method to extract the information about oov tokens, that was logged but not
returned as a result from function. I present the results of the evaluation below, but
I want to emphesise that in case of the second model 35\% of words were not in vocabulary.

\begin{figure}[h]
	\centering
	\input{./analogies_table_1.tex}
	\caption{Accuracy scores, evaluation on Google Analogies dataset.}
\end{figure}


\subsection{Analysis}
On both test sets the clear winner is the model trained on \textit{Common Crawl}. There is a
simple explanation for that fact, this model was trained on the largest number of data which really
matters in case of word embeddings. 

\section{ Training a word embedding model on in-domain data}
I have trained two models using \lstinline{train_word2vec.py}, the models were trained
with skipGram algorithm, window size of 2 and vector sizes of \textbf{300} (model1) and \textbf{100} (model2).
Both models were trained for 2 iterations.

\subsection{Top most frequent words}
I have compared the most frequent words from models trained on \textit{SignalMedia},
\textit{CoNLL17}, \textit{Oil and Gas} and \textit{Common Crawl}. The most common words in models trained on large corporas are either interpunction signs
or stop words. The explanation is that the large amount of data is more difficult to process
and the quality of it drops. The models achieve good results because the amount of data
compensates its quality. In report I include only 10 most common but file \lstinline{results/top_words.csv}
contains all 20.


\begin{figure}
	\centering
	\scalebox{0.92}{\input{./top_words.tex}}
	\caption{Most common words in models}
\end{figure}

\clearpage

\subsection{Intrinsic Evaluation}
I have evaluated both models in the same manner as the models in the previous section:
\begin{figure}[h]
	\centering
	\begin{minipage}{0.45\textwidth}
	\input{./simlex_table_2.tex}
	\end{minipage}
	\begin{minipage}{0.45\textwidth}
	\centering
	\input{./simlex_oov_table_2.tex}
	\end{minipage}
	\caption{Pearson correlation and percentage of OOV words on SimLex999 dataset.}
\end{figure}

Interestingly correlation on nouns were as good as in \textit{Common Crawl} model, even
though this model was trained on very little data. The total correlation is not really comparable,
since 90\% of verbs, which loweres the result were out of vocabulary. Comparing two models trained
on in-domain data it can be seen that increasing vector size improves the results.

\begin{figure}[h]
	\centering
	\input{./analogies_table_2.tex}
	\caption{Accuracy scores, evaluation on Google Analogies dataset.}
\end{figure}

It is hard to get useful information from evaluating the models on analogies dataset,
since almost all the classes contained words that were not in the vocabulary of this models.
The only meaningful result was from evaluating on \textit{family} section, on which model performes
worse than pre-trained embeddings. The reason for that is that the domain on which the model was 
trained is much different from the categories in this dataset.

\section{Document classification with word embeddings}
% \begin{figure}[h]
	% \centering
% \input{./similarities_classify.tex}
% \end{figure}

% \begin{figure}[h]
% 	\centering
% 	\begin{minipage}{0.4\textwidth}
%         \centering
% 		\input{./similarities_classify.tex}
% 		\caption{No POS tags.}
% 	\end{minipage}
	% \begin{minipage}{0.4\textwidth}
    %     \centering
	% 	\input{./similarities_current.tex}
	% 	\caption{With POS tags.}
	% \end{minipage}
% \end{figure}

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

% \section{Problem title} % Numbered section

% In hac habitasse platea dictumst. Curabitur mattis elit sit amet justo luctus vestibulum. In hac habitasse platea dictumst. Pellentesque lobortis justo enim, a condimentum massa tempor eu. Ut quis nulla a quam pretium eleifend nec eu nisl. Nam cursus porttitor eros, sed luctus ligula convallis quis. Nam convallis, ligula in auctor euismod, ligula mauris fringilla tellus, et egestas mauris odio eget diam. Praesent sodales in ipsum eu dictum.

%------------------------------------------------

% \subsection{Theoretical viewpoint}

% Maecenas consectetur metus at tellus finibus condimentum. Proin arcu lectus, ultrices non tincidunt et, tincidunt ut quam. Integer luctus posuere est, non maximus ante dignissim quis. Nunc a cursus erat. Curabitur suscipit nibh in tincidunt sagittis. Nam malesuada vestibulum quam id gravida. Proin ut dapibus velit. Vestibulum eget quam quis ipsum semper convallis. Duis consectetur nibh ac diam dignissim, id condimentum enim dictum. Nam aliquet ligula eu magna pellentesque, nec sagittis leo lobortis. Aenean tincidunt dignissim egestas. Morbi efficitur risus ante, id tincidunt odio pulvinar vitae.

% Curabitur tempus hendrerit nulla. Donec faucibus lobortis nibh pharetra sagittis. Sed magna sem, posuere eget sem vitae, finibus consequat libero. Cras aliquet sagittis erat ut semper. Aenean vel enim ipsum. Fusce ut felis at eros sagittis bibendum mollis lobortis libero. Donec laoreet nisl vel risus lacinia elementum non nec lacus. Nullam luctus, nulla volutpat ultricies ultrices, quam massa placerat augue, ut fringilla urna lectus nec nibh. Vestibulum efficitur condimentum orci a semper. Pellentesque ut metus pretium lacus maximus semper. Sed tellus augue, consectetur rhoncus eleifend vel, imperdiet nec turpis. Nulla ligula ante, malesuada quis orci a, ultricies blandit elit.

% Numbered question, with subquestions in an enumerate environment
% \begin{question}
	% Quisque ullamcorper placerat ipsum. Cras nibh. Morbi vel justo vitae lacus tincidunt ultrices. Lorem ipsum dolor sit amet, consectetuer adipiscing elit.

	% Subquestions numbered with letters
% 	\begin{enumerate}[(a)]
% 		\item Do this.
% 		\item Do that.
% 		\item Do something else.
% 	\end{enumerate}
% \end{question}
	
%------------------------------------------------

% \subsection{Algorithmic issues}

% In malesuada ullamcorper urna, sed dapibus diam sollicitudin non. Donec elit odio, accumsan ac nisl a, tempor imperdiet eros. Donec porta tortor eu risus consequat, a pharetra tortor tristique. Morbi sit amet laoreet erat. Morbi et luctus diam, quis porta ipsum. Quisque libero dolor, suscipit id facilisis eget, sodales volutpat dolor. Nullam vulputate interdum aliquam. Mauris id convallis erat, ut vehicula neque. Sed auctor nibh et elit fringilla, nec ultricies dui sollicitudin. Vestibulum vestibulum luctus metus venenatis facilisis. Suspendisse iaculis augue at vehicula ornare. Sed vel eros ut velit fermentum porttitor sed sed massa. Fusce venenatis, metus a rutrum sagittis, enim ex maximus velit, id semper nisi velit eu purus.

% \begin{center}
% 	\begin{minipage}{0.5\linewidth} % Adjust the minipage width to accomodate for the length of algorithm lines
% 		\begin{algorithm}[H]
% 			\KwIn{$(a, b)$, two floating-point numbers}  % Algorithm inputs
% 			\KwResult{$(c, d)$, such that $a+b = c + d$} % Algorithm outputs/results
% 			\medskip
% 			\If{$\vert b\vert > \vert a\vert$}{
% 				exchange $a$ and $b$ \;
% 			}
% 			$c \leftarrow a + b$ \;
% 			$z \leftarrow c - a$ \;
% 			$d \leftarrow b - z$ \;
% 			{\bf return} $(c,d)$ \;
% 			\caption{\texttt{FastTwoSum}} % Algorithm name
% 			\label{alg:fastTwoSum}   % optional label to refer to
% 		\end{algorithm}
% 	\end{minipage}
% \end{center}

% Fusce varius orci ac magna dapibus porttitor. In tempor leo a neque bibendum sollicitudin. Nulla pretium fermentum nisi, eget sodales magna facilisis eu. Praesent aliquet nulla ut bibendum lacinia. Donec vel mauris vulputate, commodo ligula ut, egestas orci. Suspendisse commodo odio sed hendrerit lobortis. Donec finibus eros erat, vel ornare enim mattis et.

% % Numbered question, with an optional title
% \begin{question}[\itshape (with optional title)]
% 	In congue risus leo, in gravida enim viverra id. Donec eros mauris, bibendum vel dui at, tempor commodo augue. In vel lobortis lacus. Nam ornare ullamcorper mauris vel molestie. Maecenas vehicula ornare turpis, vitae fringilla orci consectetur vel. Nam pulvinar justo nec neque egestas tristique. Donec ac dolor at libero congue varius sed vitae lectus. Donec et tristique nulla, sit amet scelerisque orci. Maecenas a vestibulum lectus, vitae gravida nulla. Proin eget volutpat orci. Morbi eu aliquet turpis. Vivamus molestie urna quis tempor tristique. Proin hendrerit sem nec tempor sollicitudin.
% \end{question}

% Mauris interdum porttitor fringilla. Proin tincidunt sodales leo at ornare. Donec tempus magna non mauris gravida luctus. Cras vitae arcu vitae mauris eleifend scelerisque. Nam sem sapien, vulputate nec felis eu, blandit convallis risus. Pellentesque sollicitudin venenatis tincidunt. In et ipsum libero. Nullam tempor ligula a massa convallis pellentesque.

%----------------------------------------------------------------------------------------
%	PROBLEM 2
%----------------------------------------------------------------------------------------

% \section{Implementation}

% Proin lobortis efficitur dictum. Pellentesque vitae pharetra eros, quis dignissim magna. Sed tellus leo, semper non vestibulum vel, tincidunt eu mi. Aenean pretium ut velit sed facilisis. Ut placerat urna facilisis dolor suscipit vehicula. Ut ut auctor nunc. Nulla non massa eros. Proin rhoncus arcu odio, eu lobortis metus sollicitudin eu. Duis maximus ex dui, id bibendum diam dignissim id. Aliquam quis lorem lorem. Phasellus sagittis aliquet dolor, vulputate cursus dolor convallis vel. Suspendisse eu tellus feugiat, bibendum lectus quis, fermentum nunc. Nunc euismod condimentum magna nec bibendum. Curabitur elementum nibh eu sem cursus, eu aliquam leo rutrum. Sed bibendum augue sit amet pharetra ullamcorper. Aenean congue sit amet tortor vitae feugiat.

% In congue risus leo, in gravida enim viverra id. Donec eros mauris, bibendum vel dui at, tempor commodo augue. In vel lobortis lacus. Nam ornare ullamcorper mauris vel molestie. Maecenas vehicula ornare turpis, vitae fringilla orci consectetur vel. Nam pulvinar justo nec neque egestas tristique. Donec ac dolor at libero congue varius sed vitae lectus. Donec et tristique nulla, sit amet scelerisque orci. Maecenas a vestibulum lectus, vitae gravida nulla. Proin eget volutpat orci. Morbi eu aliquet turpis. Vivamus molestie urna quis tempor tristique. Proin hendrerit sem nec tempor sollicitudin.

% % File contents
% \begin{file}[hello.py]
% \begin{lstlisting}[language=Python]
% #! /usr/bin/python

% import sys
% sys.stdout.write("Hello World!\n")
% \end{lstlisting}
% \end{file}

% Fusce eleifend porttitor arcu, id accumsan elit pharetra eget. Mauris luctus velit sit amet est sodales rhoncus. Donec cursus suscipit justo, sed tristique ipsum fermentum nec. Ut tortor ex, ullamcorper varius congue in, efficitur a tellus. Vivamus ut rutrum nisi. Phasellus sit amet enim efficitur, aliquam nulla id, lacinia mauris. Quisque viverra libero ac magna maximus efficitur. Interdum et malesuada fames ac ante ipsum primis in faucibus. Vestibulum mollis eros in tellus fermentum, vitae tristique justo finibus. Sed quis vehicula nibh. Etiam nulla justo, pellentesque id sapien at, semper aliquam arcu. Integer at commodo arcu. Quisque dapibus ut lacus eget vulputate.

% Command-line "screenshot"
% \begin{commandline}
% 	\begin{verbatim}
% 		$ chmod +x hello.py
% 		$ ./hello.py

% 		Hello World!
% 	\end{verbatim}
% \end{commandline}

% Vestibulum sodales orci a nisi interdum tristique. In dictum vehicula dui, eget bibendum purus elementum eu. Pellentesque lobortis mattis mauris, non feugiat dolor vulputate a. Cras porttitor dapibus lacus at pulvinar. Praesent eu nunc et libero porttitor malesuada tempus quis massa. Aenean cursus ipsum a velit ultricies sagittis. Sed non leo ullamcorper, suscipit massa ut, pulvinar erat. Aliquam erat volutpat. Nulla non lacus vitae mi placerat tincidunt et ac diam. Aliquam tincidunt augue sem, ut vestibulum est volutpat eget. Suspendisse potenti. Integer condimentum, risus nec maximus elementum, lacus purus porta arcu, at ultrices diam nisl eget urna. Curabitur sollicitudin diam quis sollicitudin varius. Ut porta erat ornare laoreet euismod. In tincidunt purus dui, nec egestas dui convallis non. In vestibulum ipsum in dictum scelerisque.

% % Warning text, with a custom title
% \begin{warn}[Notice:]
%   In congue risus leo, in gravida enim viverra id. Donec eros mauris, bibendum vel dui at, tempor commodo augue. In vel lobortis lacus. Nam ornare ullamcorper mauris vel molestie. Maecenas vehicula ornare turpis, vitae fringilla orci consectetur vel. Nam pulvinar justo nec neque egestas tristique. Donec ac dolor at libero congue varius sed vitae lectus. Donec et tristique nulla, sit amet scelerisque orci. Maecenas a vestibulum lectus, vitae gravida nulla. Proin eget volutpat orci. Morbi eu aliquet turpis. Vivamus molestie urna quis tempor tristique. Proin hendrerit sem nec tempor sollicitudin.
% \end{warn}

%----------------------------------------------------------------------------------------

\end{document}
