{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code, that I have used to produce data used as input to the scripts to provide answers for the assingment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Basic operations with word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content words from the first sentence of the abstract ot the Chen and Manning *'A fast and Accurate Dependency Parser using Neural Networks'* paper. Lemmatized and tagged with universal POS tags, saved to *words.txt* file, one word per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Almost all current dependency parsers\n",
    "classify based on millions of sparse indicator\n",
    "features\"\"\"\n",
    "\n",
    "wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "words = [w.lower() for w in words]\n",
    "words = [w for w in words if not w in stop_words]\n",
    "words = [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
    "words = ['_'.join(x) for x in nltk.pos_tag(words, tagset='universal')]\n",
    "\n",
    "with open('../data/words_auto.txt', \"w\") as f:\n",
    "    for word in words:\n",
    "        print(word, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2/3: Intrinsic evaluation of pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add universal POS tags to *Google Analogies* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/analogies/analogies_semantic.txt', \"r\") as in_f,\\\n",
    "     open('../data/analogies/analogies_semantic_POS.txt', \"w\") as out_f:\n",
    "    \n",
    "    for line in in_f:\n",
    "        line = line.strip()\n",
    "        if not line.startswith(':'):\n",
    "            line = [(word, 'NN') for word in line.split()]\n",
    "            line = ' '.join(['_'.join(x) for x in line])\n",
    "        print(line, file=out_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add universal POS tags to *SimLex999*. According to the *SimLex-999: Evaluating Semantic Models\n",
    "With (Genuine) Similarity Estimation* paper there are 3 categories of POS tags: adjectives, nouns and werbs in corresponding numbers of: 111, 666, 222."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/simlex/simlex.tsv', sep='\\t')\n",
    "df_adj = df.iloc[:111]\n",
    "df_noun = df.iloc[111:777]\n",
    "df_verb = df.iloc[777:]\n",
    "\n",
    "df.columns\n",
    "df_adj_POS = df_adj.loc[:, ['#word1', 'word2']].apply(lambda x: x + '_JJ').join(df_adj.loc[:, 'SimLex999'])\n",
    "df_noun_POS = df_noun.loc[:, ['#word1', 'word2']].apply(lambda x: x + '_NN').join(df_noun.loc[:, 'SimLex999'])\n",
    "df_verb_POS = df_verb.loc[:, ['#word1', 'word2']].apply(lambda x: x + '_VB').join(df_verb.loc[:, 'SimLex999'])\n",
    "df_POS = pd.concat([df_adj_POS, df_noun_POS, df_verb_POS])\n",
    "\n",
    "df_adj.to_csv('../data/simlex/simlex_adj.tsv', sep='\\t', index=False)\n",
    "df_noun.to_csv('../data/simlex/simlex_noun.tsv', sep='\\t', index=False)\n",
    "df_verb.to_csv('../data/simlex/simlex_verb.tsv', sep='\\t', index=False)\n",
    "\n",
    "df_adj_POS.to_csv('../data/simlex/simlex_adj_POS.tsv', sep='\\t', index=False)\n",
    "df_noun_POS.to_csv('../data/simlex/simlex_noun_POS.tsv', sep='\\t', index=False)\n",
    "df_verb_POS.to_csv('../data/simlex/simlex_verb_POS.tsv', sep='\\t', index=False)\n",
    "\n",
    "df_POS.to_csv('../data/simlex/simlex_POS.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:inf5820]",
   "language": "python",
   "name": "conda-env-inf5820-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
